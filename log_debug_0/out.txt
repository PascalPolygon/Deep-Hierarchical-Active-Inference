11:36:41

=== Loading experiment [device: cpu] ===

{'action_noise': None,
 'action_noise_scale': 0.1,
 'action_repeat': 3,
 'batch_size': 50,
 'context_length': 2,
 'coverage': False,
 'ensemble_size': 10,
 'env_name': 'Pendulum-v0',
 'epsilon': 1e-08,
 'expl_scale': 1.0,
 'expl_strategy': 'information',
 'grad_clip_norm': 1000,
 'hidden_size': 64,
 'learning_rate': 0.001,
 'logdir': 'debug',
 'max_episode_len': 10,
 'n_candidates': 500,
 'n_episodes': 3,
 'n_seed_episodes': 5,
 'n_train_epochs': 200,
 'optimisation_iters': 5,
 'plan_horizon': 5,
 'record_every': None,
 'reward_scale': 1.0,
 'seed': 0,
 'strategy': 'information',
 'top_candidates': 50,
 'use_exploration': True,
 'use_mean': False,
 'use_reward': True}
Low-level transition added: state=[-0.94223519 -0.33495202  0.93078187], goal=[-0.90963716 -0.4154037   0.37925835], action=[0.6145334], reward=-23.112198822411933, next_state=[-0.90963716 -0.4154037   0.37925835], next_goal=[-0.90963716 -0.4154037   0.37925835]
Low-level transition added: state=[-0.90963716 -0.4154037   0.37925835], goal=[-0.89612897 -0.44379373  0.11608952], action=[1.5535517], reward=-21.89979636442142, next_state=[-0.89612897 -0.44379373  0.11608952], next_goal=[-0.89612897 -0.44379373  0.11608952]
Low-level transition added: state=[-0.89612897 -0.44379373  0.11608952], goal=[-0.91682558 -0.39928793 -0.54148422], action=[0.7164667], reward=-21.738518708354544, next_state=[-0.91682558 -0.39928793 -0.54148422], next_goal=[-0.91682558 -0.39928793 -0.54148422]
Low-level transition added: state=[-0.91682558 -0.39928793 -0.54148422], goal=[-0.93378064 -0.35784593 -0.89559984], action=[-0.3643312], reward=-7.487009243323405, next_state=[-0.93378064 -0.35784593 -0.89559984], next_goal=[-0.93378064 -0.35784593 -0.89559984]
Episode 1/5 complete.
Low-level transition added: state=[ 0.67212466  0.74043801 -0.41996531], goal=[0.5160204  0.85657629 2.17934555], action=[1.9587379], reward=-2.4858646740592407, next_state=[0.5160204  0.85657629 2.17934555], next_goal=[0.5160204  0.85657629 2.17934555]
Low-level transition added: state=[0.5160204  0.85657629 2.17934555], goal=[0.05261413 0.99861492 3.8485072 ], action=[-0.8653736], reward=-6.427197975927192, next_state=[0.05261413 0.99861492 3.8485072 ], next_goal=[0.05261413 0.99861492 3.8485072 ]
Low-level transition added: state=[0.05261413 0.99861492 3.8485072 ], goal=[-0.62920433  0.77723993  5.37660682], action=[-1.4396205], reward=-15.066283711722612, next_state=[-0.62920433  0.77723993  5.37660682], next_goal=[-0.62920433  0.77723993  5.37660682]
Low-level transition added: state=[-0.62920433  0.77723993  5.37660682], goal=[-0.83159674  0.55537992  6.02894995], action=[0.46275455], reward=-7.959470289063877, next_state=[-0.83159674  0.55537992  6.02894995], next_goal=[-0.83159674  0.55537992  6.02894995]
Episode 2/5 complete.
Low-level transition added: state=[-0.79974405 -0.60034112  0.3461527 ], goal=[-0.88096427 -0.4731828  -1.66248304], action=[-1.5775267], reward=-19.27875089032868, next_state=[-0.88096427 -0.4731828  -1.66248304], next_goal=[-0.88096427 -0.4731828  -1.66248304]
Low-level transition added: state=[-0.88096427 -0.4731828  -1.66248304], goal=[-0.97335323 -0.22931092 -1.72371148], action=[1.8308496], reward=-23.376903663411337, next_state=[-0.97335323 -0.22931092 -1.72371148], next_goal=[-0.97335323 -0.22931092 -1.72371148]
Low-level transition added: state=[-0.97335323 -0.22931092 -1.72371148], goal=[-0.99999879 -0.00155694 -1.37665433], action=[1.5163393], reward=-27.686527523825745, next_state=[-0.99999879 -0.00155694 -1.37665433], next_goal=[-0.99999879 -0.00155694 -1.37665433]
Low-level transition added: state=[-0.99999879 -0.00155694 -1.37665433], goal=[-0.99779589  0.06635784 -1.3592717 ], action=[0.12366893], reward=-10.049357264353862, next_state=[-0.99779589  0.06635784 -1.3592717 ], next_goal=[-0.99779589  0.06635784 -1.3592717 ]
Episode 3/5 complete.
Low-level transition added: state=[ 0.78073179 -0.62486628  0.33969213], goal=[ 0.76207567 -0.64748797 -0.46485447], action=[1.3396524], reward=-1.3913025825515162, next_state=[ 0.76207567 -0.64748797 -0.46485447], next_goal=[ 0.76207567 -0.64748797 -0.46485447]
Low-level transition added: state=[ 0.76207567 -0.64748797 -0.46485447], goal=[ 0.53509927 -0.84478919 -2.82898357], action=[-1.7521164], reward=-2.4051239771188477, next_state=[ 0.53509927 -0.84478919 -2.82898357], next_goal=[ 0.53509927 -0.84478919 -2.82898357]
Low-level transition added: state=[ 0.53509927 -0.84478919 -2.82898357], goal=[-0.12539215 -0.99210726 -5.54290477], action=[-1.4280701], reward=-8.705746626994998, next_state=[-0.12539215 -0.99210726 -5.54290477], next_goal=[-0.12539215 -0.99210726 -5.54290477]
Low-level transition added: state=[-0.12539215 -0.99210726 -5.54290477], goal=[-0.4251785  -0.90510952 -6.26871994], action=[0.1217685], reward=-5.950572284740293, next_state=[-0.4251785  -0.90510952 -6.26871994], next_goal=[-0.4251785  -0.90510952 -6.26871994]
Episode 4/5 complete.
Low-level transition added: state=[-0.93014847 -0.36718363  0.05397449], goal=[-0.95578385 -0.29407013 -0.7885809 ], action=[-0.11440635], reward=-23.257337109306487, next_state=[-0.95578385 -0.29407013 -0.7885809 ], next_goal=[-0.95578385 -0.29407013 -0.7885809 ]
Low-level transition added: state=[-0.95578385 -0.29407013 -0.7885809 ], goal=[-0.98232946 -0.1871599  -0.68022731], action=[1.5259432], reward=-25.09765138668462, next_state=[-0.98232946 -0.1871599  -0.68022731], next_goal=[-0.98232946 -0.1871599  -0.68022731]
Low-level transition added: state=[-0.98232946 -0.1871599  -0.68022731], goal=[-0.99739261 -0.07216638 -0.79195911], action=[0.5003301], reward=-27.00924449266192, next_state=[-0.99739261 -0.07216638 -0.79195911], next_goal=[-0.99739261 -0.07216638 -0.79195911]
Low-level transition added: state=[-0.99739261 -0.07216638 -0.79195911], goal=[-0.99915808 -0.04102597 -0.62383363], action=[1.4816685], reward=-9.485907450155448, next_state=[-0.99915808 -0.04102597 -0.62383363], next_goal=[-0.99915808 -0.04102597 -0.62383363]
Episode 5/5 complete.

Collected seeds: [5 episodes | 20 frames]

=== Episode 1 ===
Training on [20/60] data points
> High-Level Train epoch 20 [ensemble 154.09 | reward 1061.97]
> Low-Level Train epoch 20 [action 0.20]
> High-Level Train epoch 40 [ensemble 71.02 | reward 917.41]
> Low-Level Train epoch 40 [action 0.18]
> High-Level Train epoch 60 [ensemble 21.23 | reward 687.39]
> Low-Level Train epoch 60 [action 0.17]
> High-Level Train epoch 80 [ensemble 5.03 | reward 483.86]
> Low-Level Train epoch 80 [action 0.17]
> High-Level Train epoch 100 [ensemble -2.08 | reward 359.90]
> Low-Level Train epoch 100 [action 0.17]
> High-Level Train epoch 120 [ensemble -4.48 | reward 258.45]
> Low-Level Train epoch 120 [action 0.18]
> High-Level Train epoch 140 [ensemble -7.12 | reward 193.52]
> Low-Level Train epoch 140 [action 0.17]
> High-Level Train epoch 160 [ensemble -8.48 | reward 162.51]
> Low-Level Train epoch 160 [action 0.18]
> High-Level Train epoch 180 [ensemble -10.31 | reward 149.15]
> Low-Level Train epoch 180 [action 0.19]
> High-Level Train epoch 200 [ensemble -11.35 | reward 142.41]
> Low-Level Train epoch 200 [action 0.19]
Ensemble losses: [-11.350032806396484]
Reward losses: [142.41004943847656]
Action losses: [0.18241283297538757, 0.17174458503723145, 0.16599074006080627, 0.20502179861068726, 0.17639963328838348, 0.16101934015750885, 0.20166629552841187, 0.17792071402072906, 0.1831691712141037, 0.16619069874286652, 0.19802308082580566, 0.18613840639591217, 0.19703906774520874, 0.17874132096767426, 0.22169546782970428, 0.19609187543392181, 0.16108888387680054, 0.19954149425029755, 0.1940285712480545, 0.19425952434539795]

=== Collecting data [1] ===
Episode complete. Total reward: -76.63235913572895, Total steps: 4
Rewards -76.63 / Steps 4.00
Reward stats:
 {'max': '-57.85', 'mean': '-84.13', 'min': '-159.72', 'std': '19.06'}
Information gain stats:
 {'max': '9.30', 'mean': '3.29', 'min': '-2.54', 'std': '1.61'}
Episode time 8.93
Saved _metrics_

=== Episode 2 ===
Training on [24/72] data points
> High-Level Train epoch 20 [ensemble 60.58 | reward 1213.91]
> Low-Level Train epoch 20 [action 0.23]
> High-Level Train epoch 40 [ensemble 22.03 | reward 1119.91]
> Low-Level Train epoch 40 [action 0.18]
> High-Level Train epoch 60 [ensemble 2.64 | reward 942.17]
> Low-Level Train epoch 60 [action 0.17]
> High-Level Train epoch 80 [ensemble -6.17 | reward 708.80]
> Low-Level Train epoch 80 [action 0.17]
> High-Level Train epoch 100 [ensemble -9.74 | reward 519.27]
> Low-Level Train epoch 100 [action 0.16]
> High-Level Train epoch 120 [ensemble -13.10 | reward 370.76]
> Low-Level Train epoch 120 [action 0.16]
> High-Level Train epoch 140 [ensemble -14.46 | reward 254.26]
> Low-Level Train epoch 140 [action 0.17]
> High-Level Train epoch 160 [ensemble -15.85 | reward 180.49]
> Low-Level Train epoch 160 [action 0.17]
> High-Level Train epoch 180 [ensemble -18.25 | reward 144.42]
> Low-Level Train epoch 180 [action 0.17]
> High-Level Train epoch 200 [ensemble -19.83 | reward 127.95]
> Low-Level Train epoch 200 [action 0.18]
Ensemble losses: [-19.826454162597656]
Reward losses: [127.94546508789062]
Action losses: [0.18873231112957, 0.16088104248046875, 0.18538029491901398, 0.18107204139232635, 0.18829670548439026, 0.1684548705816269, 0.15890102088451385, 0.21827785670757294, 0.16052070260047913, 0.20116767287254333, 0.1817006766796112, 0.16951949894428253, 0.19877450168132782, 0.1730353981256485, 0.20823943614959717, 0.15293775498867035, 0.1973961889743805, 0.19715291261672974, 0.1533755660057068, 0.17051897943019867]

=== Collecting data [2] ===
Episode complete. Total reward: -0.4994044099251586, Total steps: 4
Rewards -0.50 / Steps 4.00
Reward stats:
 {'max': '-32.19', 'mean': '-46.30', 'min': '-103.26', 'std': '13.03'}
Information gain stats:
 {'max': '8.83', 'mean': '2.00', 'min': '-2.97', 'std': '1.44'}
Episode time 9.73
Saved _metrics_